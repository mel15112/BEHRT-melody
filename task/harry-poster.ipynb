{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec71fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8d0aa",
   "metadata": {},
   "source": [
    "###  This Notebook is used to compare BEHART vs BOC+lr to see how it works and adding some attention explaination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from common.common import create_folder,load_obj\n",
    "# from data import bert,dataframe,utils\n",
    "from dataLoader.utils import seq_padding,code2index, position_idx, index_seg\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from model.utils import age_vocab\n",
    "from model import optimiser\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# from data.utils import seq_padding, index_seg, position_idx, age_vocab, random_mask, code2index\n",
    "# from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28fff58",
   "metadata": {},
   "source": [
    "### Adding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'vocab': '../../outputs/vocab',\n",
    "    'train': '../../outputs/nextvisit_train_idx.parquet',\n",
    "    'test':  '../../outputs/nextvisit_test_idx.parquet',\n",
    "}\n",
    "\n",
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 128,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "    'output_dir': '../../outputs/ckpts',\n",
    "    'best_name': 'nextvisit_12m.pt',\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 100,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5\n",
    "}\n",
    "\n",
    "pretrainModel = \"../../outputs/ckpts/mlm_bert.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a985dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(global_params['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_special_tokens(token2idx):\n",
    "    base = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "    for sp in base:\n",
    "        if sp not in token2idx:\n",
    "            token2idx[sp] = len(token2idx)\n",
    "\n",
    "    alias = {'UNK':'[UNK]','PAD':'[PAD]','CLS':'[CLS]','SEP':'[SEP]','MASK':'[MASK]'}\n",
    "    for a,b in alias.items():\n",
    "        if a not in token2idx:\n",
    "            token2idx[a] = token2idx[b]\n",
    "    return token2idx\n",
    "\n",
    "BertVocab['token2idx'] = ensure_special_tokens(BertVocab['token2idx'])\n",
    "\n",
    "def format_label_vocab(token2idx):\n",
    "    token2idx = token2idx.copy()\n",
    "    # remove specials from label set\n",
    "    for sp in ['PAD','[PAD]','SEP','[SEP]','CLS','[CLS]','MASK','[MASK]','UNK','[UNK]']:\n",
    "        if sp in token2idx:\n",
    "            del token2idx[sp]\n",
    "    labelVocab = {tok:i for i,tok in enumerate(token2idx.keys())}\n",
    "    return labelVocab\n",
    "\n",
    "Vocab_diag = format_label_vocab(BertVocab['token2idx'])\n",
    "\n",
    "print(\"✅ word vocab size:\", len(BertVocab['token2idx']))\n",
    "print(\"✅ label vocab size:\", len(Vocab_diag))\n",
    "print(\"✅ UNK id:\", BertVocab['token2idx']['UNK'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed446beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell: MultiLabelBinarizer =====\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=list(Vocab_diag.values()))\n",
    "mlb.fit([[i] for i in list(Vocab_diag.values())])\n",
    "\n",
    "print(\"✅ mlb fitted, n_labels =\", len(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.1, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'word':True,\n",
    "    'seg':True,\n",
    "    'age':True,\n",
    "    'position': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a6685",
   "metadata": {},
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def code2index_safe(tokens, token2idx):\n",
    "    \"\"\"\n",
    "    tokens: list/np.array of tokens (strings)\n",
    "    token2idx: dict token->id (may have UNK or [UNK] or neither)\n",
    "    \"\"\"\n",
    "    # normalize to list\n",
    "    if isinstance(tokens, np.ndarray):\n",
    "        tokens_list = tokens.tolist()\n",
    "    else:\n",
    "        tokens_list = list(tokens)\n",
    "\n",
    "    # pick unk id safely\n",
    "    if \"UNK\" in token2idx:\n",
    "        unk_id = token2idx[\"UNK\"]\n",
    "    elif \"[UNK]\" in token2idx:\n",
    "        unk_id = token2idx[\"[UNK]\"]\n",
    "    else:\n",
    "        # last resort: use 0\n",
    "        unk_id = 0\n",
    "\n",
    "    out = []\n",
    "    for t in tokens_list:\n",
    "        # ensure token is a python str key, not numpy scalar\n",
    "        if not isinstance(t, str):\n",
    "            t = str(t)\n",
    "        out.append(token2idx.get(t, unk_id))\n",
    "    return tokens_list, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class NextVisit(Dataset):\n",
    "    def __init__(self, token2idx, diag2idx, age2idx, dataframe, max_len, max_age=110, min_visit=5):\n",
    "        self.vocab = token2idx\n",
    "        self.label_vocab = diag2idx\n",
    "        self.age2idx = age2idx\n",
    "        self.max_len = int(max_len)\n",
    "\n",
    "        self.code = dataframe[\"code\"]\n",
    "        self.age = dataframe[\"age\"]\n",
    "        self.label = dataframe[\"label\"]\n",
    "\n",
    "        if \"patid\" in dataframe.columns:\n",
    "            self.patid = dataframe[\"patid\"]\n",
    "        elif \"subject_id\" in dataframe.columns:\n",
    "            self.patid = dataframe[\"subject_id\"]\n",
    "        else:\n",
    "            self.patid = dataframe.index\n",
    "\n",
    "        # pad id (try PAD/[PAD], else 0)\n",
    "        self.pad_id = self.vocab.get(\"PAD\", self.vocab.get(\"[PAD]\", 0))\n",
    "\n",
    "        # (optional) make sure special aliases exist in THIS dict\n",
    "        if \"UNK\" not in self.vocab and \"[UNK]\" in self.vocab:\n",
    "            self.vocab[\"UNK\"] = self.vocab[\"[UNK]\"]\n",
    "        if \"PAD\" not in self.vocab and \"[PAD]\" in self.vocab:\n",
    "            self.vocab[\"PAD\"] = self.vocab[\"[PAD]\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.code)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        codes = list(self.code[index])\n",
    "        ages  = list(self.age[index])\n",
    "        label = list(self.label[index])\n",
    "        patid = int(self.patid[index])\n",
    "\n",
    "        # truncate to max_len-1 then add CLS\n",
    "        codes = codes[-(self.max_len - 1):]\n",
    "        ages  = ages[-(self.max_len - 1):]\n",
    "\n",
    "        if len(codes) == 0:\n",
    "            codes = [\"CLS\"]\n",
    "            ages  = [0]\n",
    "        else:\n",
    "            if codes[0] != \"SEP\":\n",
    "                codes = [\"CLS\"] + codes\n",
    "                ages  = [ages[0] if len(ages) > 0 else 0] + ages\n",
    "            else:\n",
    "                codes[0] = \"CLS\"\n",
    "\n",
    "        # visit_ids\n",
    "        visit_ids = []\n",
    "        v = 0\n",
    "        for t in codes:\n",
    "            visit_ids.append(v)\n",
    "            if t == \"SEP\":\n",
    "                v += 1\n",
    "\n",
    "        # keep raw tokens\n",
    "        code_tokens = codes[:]\n",
    "\n",
    "        # SAFE convert to ids (no KeyError)\n",
    "        _, code_ids  = code2index_safe(np.array(codes, dtype=object), self.vocab)\n",
    "        _, label_ids = code2index_safe(label, self.label_vocab)\n",
    "\n",
    "        # robust age -> id mapping\n",
    "        mapped_ages = []\n",
    "        for a in ages:\n",
    "            try:\n",
    "                ai = int(a)\n",
    "            except Exception:\n",
    "                ai = 0\n",
    "            if ai in self.age2idx:\n",
    "                mapped_ages.append(ai)\n",
    "            elif str(ai) in self.age2idx:\n",
    "                mapped_ages.append(str(ai))\n",
    "            else:\n",
    "                mapped_ages.append(0)\n",
    "\n",
    "        age_ids = seq_padding(mapped_ages, self.max_len, token2idx=self.age2idx)\n",
    "\n",
    "        # pad to max_len\n",
    "        code_ids  = seq_padding(code_ids,  self.max_len, symbol=self.pad_id)\n",
    "        label_ids = seq_padding(label_ids, self.max_len, symbol=-1)\n",
    "\n",
    "        if len(visit_ids) < self.max_len:\n",
    "            visit_ids = visit_ids + [-1] * (self.max_len - len(visit_ids))\n",
    "        else:\n",
    "            visit_ids = visit_ids[:self.max_len]\n",
    "\n",
    "        if len(code_tokens) < self.max_len:\n",
    "            code_tokens = code_tokens + [\"PAD\"] * (self.max_len - len(code_tokens))\n",
    "        else:\n",
    "            code_tokens = code_tokens[:self.max_len]\n",
    "\n",
    "        position = position_idx(code_ids)\n",
    "        segment  = index_seg(code_ids)\n",
    "\n",
    "        # attMask\n",
    "        attMask = (np.array(code_ids) != self.pad_id).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.LongTensor(age_ids),\n",
    "            torch.LongTensor(code_ids),\n",
    "            torch.LongTensor(position),\n",
    "            torch.LongTensor(segment),\n",
    "            torch.FloatTensor(attMask),\n",
    "            torch.LongTensor(label_ids),\n",
    "            torch.LongTensor([patid]),\n",
    "            torch.LongTensor(visit_ids),\n",
    "            code_tokens\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4fc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings=config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(config.seg_vocab_size, config.hidden_size)\n",
    "        self.age_embeddings = nn.Embedding(config.age_vocab_size, config.hidden_size)\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).from_pretrained(\n",
    "            embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = Bert.modeling.BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, age_ids=None, seg_ids=None, posi_ids=None):\n",
    "        if seg_ids is None: seg_ids = torch.zeros_like(word_ids)\n",
    "        if age_ids is None: age_ids = torch.zeros_like(word_ids)\n",
    "        if posi_ids is None: posi_ids = torch.zeros_like(word_ids)\n",
    "\n",
    "        word_embed = self.word_embeddings(word_ids)\n",
    "        segment_embed = self.segment_embeddings(seg_ids)\n",
    "        age_embed = self.age_embeddings(age_ids)\n",
    "        posi_embed = self.posi_embeddings(posi_ids)\n",
    "\n",
    "        embeddings = word_embed + segment_embed + age_embed + posi_embed\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "        for pos in range(max_position_embedding):\n",
    "            for i in range(0, hidden_size, 2):\n",
    "                lookup_table[pos, i] = np.sin(pos/(10000**(2*i/hidden_size)))\n",
    "            for i in range(1, hidden_size, 2):\n",
    "                lookup_table[pos, i] = np.cos(pos/(10000**(2*i/hidden_size)))\n",
    "        return torch.tensor(lookup_table)\n",
    "\n",
    "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = Bert.modeling.BertEncoder(config)\n",
    "        self.pooler = Bert.modeling.BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, age_ids, seg_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output, extended_attention_mask, output_all_encoded_layers=False)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        return sequence_output, pooled_output\n",
    "\n",
    "class BertForMultiLabelPrediction(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, age_ids, seg_ids, posi_ids, attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss, logits\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0aca61",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertVocab[\"token2idx\"]\n",
    "\n",
    "def force_alias(tok):\n",
    "    # ensure base specials exist\n",
    "    if \"[UNK]\" not in tok: tok[\"[UNK]\"] = len(tok)\n",
    "    if \"[PAD]\" not in tok: tok[\"[PAD]\"] = len(tok)\n",
    "    if \"[CLS]\" not in tok: tok[\"[CLS]\"] = len(tok)\n",
    "    if \"[SEP]\" not in tok: tok[\"[SEP]\"] = len(tok)\n",
    "    if \"[MASK]\" not in tok: tok[\"[MASK]\"] = len(tok)\n",
    "\n",
    "    # ensure aliases exist (THIS FIXES YOUR ERROR)\n",
    "    tok[\"UNK\"]  = tok.get(\"UNK\",  tok[\"[UNK]\"])\n",
    "    tok[\"PAD\"]  = tok.get(\"PAD\",  tok[\"[PAD]\"])\n",
    "    tok[\"CLS\"]  = tok.get(\"CLS\",  tok[\"[CLS]\"])\n",
    "    tok[\"SEP\"]  = tok.get(\"SEP\",  tok[\"[SEP]\"])\n",
    "    tok[\"MASK\"] = tok.get(\"MASK\", tok[\"[MASK]\"])\n",
    "    return tok\n",
    "\n",
    "BertVocab[\"token2idx\"] = force_alias(tok)\n",
    "\n",
    "print(\"✅ has UNK:\", \"UNK\" in BertVocab[\"token2idx\"], \" id=\", BertVocab[\"token2idx\"][\"UNK\"])\n",
    "print(\"✅ has PAD:\", \"PAD\" in BertVocab[\"token2idx\"], \" id=\", BertVocab[\"token2idx\"][\"PAD\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_partial(model, ckpt_path, device='cpu'):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    if isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "        ckpt = ckpt[\"state_dict\"]\n",
    "\n",
    "    model_state = model.state_dict()\n",
    "    new_state = {}\n",
    "\n",
    "    for k, v in ckpt.items():\n",
    "        if k not in model_state:\n",
    "            continue\n",
    "        v = v.to(model_state[k].dtype)\n",
    "\n",
    "        if \"word_embeddings.weight\" in k:\n",
    "            new_embed = model_state[k].clone()\n",
    "            n_copy = min(v.shape[0], new_embed.shape[0])\n",
    "            new_embed[:n_copy] = v[:n_copy]\n",
    "            new_state[k] = new_embed\n",
    "            continue\n",
    "\n",
    "        if \"posi_embeddings.weight\" in k:\n",
    "            new_pos = model_state[k].clone()\n",
    "            n_copy = min(v.shape[0], new_pos.shape[0])\n",
    "            new_pos[:n_copy] = v[:n_copy]\n",
    "            new_state[k] = new_pos\n",
    "            continue\n",
    "\n",
    "        if model_state[k].shape == v.shape:\n",
    "            new_state[k] = v\n",
    "\n",
    "    model_state.update(new_state)\n",
    "    model.load_state_dict(model_state)\n",
    "    print(\"✅ MLM partial load done.\")\n",
    "    return model\n",
    "\n",
    "def format_label_vocab(token2idx):\n",
    "    token2idx = token2idx.copy()\n",
    "    for sp in ['PAD','[PAD]','SEP','[SEP]','CLS','[CLS]','MASK','[MASK]','UNK','[UNK]']:\n",
    "        if sp in token2idx:\n",
    "            del token2idx[sp]\n",
    "    return {tok:i for i,tok in enumerate(token2idx.keys())}\n",
    "\n",
    "Vocab_diag = format_label_vocab(BertVocab[\"token2idx\"])\n",
    "print(\"✅ label vocab size:\", len(Vocab_diag))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ab054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell: rebuild dfs + loaders =====\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Try reading with fastparquet engine first (works around pyarrow extension type issues).\n",
    "# Falls back to pandas default engine if fastparquet is not available or fails.\n",
    "try:\n",
    "    train_df = pd.read_parquet(file_config['train'], engine='fastparquet').reset_index(drop=True)\n",
    "    test_df  = pd.read_parquet(file_config['test'],  engine='fastparquet').reset_index(drop=True)\n",
    "except Exception as e:\n",
    "    print('fastparquet engine failed or not installed, falling back to default engine:', e)\n",
    "    train_df = pd.read_parquet(file_config['train']).reset_index(drop=True)\n",
    "    test_df  = pd.read_parquet(file_config['test']).reset_index(drop=True)\n",
    "\n",
    "train_df[\"label\"] = train_df[\"label\"].apply(lambda x: list(set(list(x))))\n",
    "test_df[\"label\"]  = test_df[\"label\"].apply(lambda x: list(set(list(x))))\n",
    "\n",
    "if \"patid\" not in train_df.columns: train_df[\"patid\"] = train_df[\"subject_id\"]\n",
    "if \"patid\" not in test_df.columns:  test_df[\"patid\"]  = test_df[\"subject_id\"]\n",
    "\n",
    "trainset = NextVisit(BertVocab['token2idx'], Vocab_diag, ageVocab, train_df, global_params['max_len_seq'])\n",
    "testset  = NextVisit(BertVocab['token2idx'], Vocab_diag, ageVocab, test_df,  global_params['max_len_seq'])\n",
    "\n",
    "trainload = DataLoader(trainset, batch_size=global_params['batch_size'], shuffle=True,  num_workers=0)\n",
    "testload  = DataLoader(testset,  batch_size=global_params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"✅ loaders ready\")\n",
    "x = trainset[0]\n",
    "print(\"✅ trainset[0] ok, fields:\", len(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e20d9",
   "metadata": {},
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import sklearn.metrics as skm\n",
    "device = global_params['device']\n",
    "print(device)\n",
    "conf = BertConfig(model_config)\n",
    "model = BertForMultiLabelPrediction(conf, num_labels=len(Vocab_diag)).to(device)\n",
    "\n",
    "model = load_pretrained_partial(model, pretrainModel, device=device).to(device)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=list(Vocab_diag.values()))\n",
    "mlb.fit([[i] for i in list(Vocab_diag.values())])\n",
    "\n",
    "optim = optimiser.adam(params=list(model.named_parameters()), config=optim_config)\n",
    "print(\"✅ model/optim ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882101b",
   "metadata": {},
   "source": [
    "### Evaluation Matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_samples(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output = sig(logits).detach().cpu().numpy()\n",
    "    label  = label.detach().cpu().numpy()\n",
    "    return skm.average_precision_score(label, output, average='samples')\n",
    "\n",
    "# ===== Cell: Train/Eval (9-field batch) =====\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "device = global_params[\"device\"]\n",
    "\n",
    "def train_one_epoch(e, log_every=200):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_steps = 0\n",
    "\n",
    "    for step, batch in enumerate(trainload):\n",
    "        # ✅ NEW: unpack 9 fields\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, label_ids, patid, visit_ids, code_tokens = batch\n",
    "\n",
    "        # label_ids 是 padding 的序列标签，不用于训练\n",
    "        # 用 raw labels -> multi-hot 才对\n",
    "        # 但是我们 dataset 里第6个返回的是 label_ids（被 safe code2index 处理过）\n",
    "        # 所以这里改成：从 label_ids 里恢复有效 label（去掉 -1）再 multi-hot\n",
    "        # label_ids shape: [B, L]\n",
    "        label_ids_np = label_ids.numpy()\n",
    "        raw_labels = []\n",
    "        for i in range(label_ids_np.shape[0]):\n",
    "            labs = [int(x) for x in label_ids_np[i].tolist() if int(x) >= 0]\n",
    "            raw_labels.append(list(set(labs)))\n",
    "\n",
    "        targets = torch.tensor(mlb.transform(raw_labels), dtype=torch.float32).to(device)\n",
    "\n",
    "        age_ids    = age_ids.to(device)\n",
    "        input_ids  = input_ids.to(device)\n",
    "        posi_ids   = posi_ids.to(device)\n",
    "        segment_ids= segment_ids.to(device)\n",
    "        attMask    = attMask.to(device)\n",
    "\n",
    "        loss, logits = model(\n",
    "            input_ids,\n",
    "            age_ids,\n",
    "            segment_ids,\n",
    "            posi_ids,\n",
    "            attention_mask=attMask,\n",
    "            labels=targets\n",
    "        )\n",
    "\n",
    "        if global_params.get(\"gradient_accumulation_steps\", 1) > 1:\n",
    "            loss = loss / global_params[\"gradient_accumulation_steps\"]\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % global_params.get(\"gradient_accumulation_steps\", 1) == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_steps += 1\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            with torch.no_grad():\n",
    "                prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "                y    = targets.detach().cpu().numpy()\n",
    "                aps  = average_precision_score(y, prob, average=\"micro\")\n",
    "            print(f\"[Train] epoch {e} step {step} | loss={loss.item():.4f} | APS(micro)={aps:.4f}\")\n",
    "\n",
    "    return total_loss / max(n_steps, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluation():\n",
    "    model.eval()\n",
    "    y_prob_list = []\n",
    "    y_true_list = []\n",
    "\n",
    "    for batch in testload:\n",
    "        # ✅ NEW: unpack 9 fields\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, label_ids, patid, visit_ids, code_tokens = batch\n",
    "\n",
    "        # same: label_ids -> raw labels -> multi-hot\n",
    "        label_ids_np = label_ids.numpy()\n",
    "        raw_labels = []\n",
    "        for i in range(label_ids_np.shape[0]):\n",
    "            labs = [int(x) for x in label_ids_np[i].tolist() if int(x) >= 0]\n",
    "            raw_labels.append(list(set(labs)))\n",
    "\n",
    "        targets = torch.tensor(mlb.transform(raw_labels), dtype=torch.float32).to(device)\n",
    "\n",
    "        age_ids     = age_ids.to(device)\n",
    "        input_ids   = input_ids.to(device)\n",
    "        posi_ids    = posi_ids.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        attMask     = attMask.to(device)\n",
    "\n",
    "        loss, logits = model(\n",
    "            input_ids,\n",
    "            age_ids,\n",
    "            segment_ids,\n",
    "            posi_ids,\n",
    "            attention_mask=attMask,\n",
    "            labels=targets\n",
    "        )\n",
    "\n",
    "        y_prob_list.append(torch.sigmoid(logits).cpu().numpy())\n",
    "        y_true_list.append(targets.cpu().numpy())\n",
    "\n",
    "    y_prob = np.vstack(y_prob_list)\n",
    "    y_true = np.vstack(y_true_list)\n",
    "\n",
    "    aps   = average_precision_score(y_true, y_prob, average=\"micro\")\n",
    "    auroc = roc_auc_score(y_true, y_prob, average=\"micro\")\n",
    "\n",
    "    return aps, auroc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_test_probs():\n",
    "    model.eval()\n",
    "    y_prob_list, y_true_list = [], []\n",
    "\n",
    "    for batch in testload:\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, label_ids, patid, visit_ids, code_tokens = batch\n",
    "\n",
    "        # label_ids -> raw labels -> multi-hot\n",
    "        label_ids_np = label_ids.numpy()\n",
    "        raw_labels = []\n",
    "        for i in range(label_ids_np.shape[0]):\n",
    "            labs = [int(x) for x in label_ids_np[i].tolist() if int(x) >= 0]\n",
    "            raw_labels.append(list(set(labs)))\n",
    "\n",
    "        targets = torch.tensor(mlb.transform(raw_labels), dtype=torch.float32).to(device)\n",
    "\n",
    "        age_ids     = age_ids.to(device)\n",
    "        input_ids   = input_ids.to(device)\n",
    "        posi_ids    = posi_ids.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        attMask     = attMask.to(device)\n",
    "\n",
    "        loss, logits = model(\n",
    "            input_ids,\n",
    "            age_ids,\n",
    "            segment_ids,\n",
    "            posi_ids,\n",
    "            attention_mask=attMask,\n",
    "            labels=targets\n",
    "        )\n",
    "\n",
    "        y_prob_list.append(torch.sigmoid(logits).cpu().numpy())\n",
    "        y_true_list.append(targets.cpu().numpy())\n",
    "\n",
    "    y_prob = np.vstack(y_prob_list)\n",
    "    y_true = np.vstack(y_true_list)\n",
    "    return y_true, y_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f370c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(global_params[\"output_dir\"], global_params[\"best_name\"])\n",
    "print(\"Loading:\", ckpt_path)\n",
    "\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Loaded trained model for explain/plotting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextVisitExplain(torch.utils.data.Dataset):\n",
    "    def __init__(self, token2idx, dataframe, max_len):\n",
    "        self.vocab = token2idx\n",
    "        self.code = dataframe[\"code\"]\n",
    "        self.age  = dataframe[\"age\"]\n",
    "        self.label = dataframe[\"label\"]\n",
    "        self.patid = dataframe[\"patid\"]\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = self.vocab[\"PAD\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        codes = list(self.code[idx])[-(self.max_len-1):]\n",
    "        ages  = list(self.age[idx])[-(self.max_len-1):]\n",
    "\n",
    "        codes = [\"CLS\"] + codes\n",
    "        ages  = [ages[0] if len(ages)>0 else 0] + ages\n",
    "\n",
    "        visit_ids = []\n",
    "        v = 0\n",
    "        for c in codes:\n",
    "            visit_ids.append(v)\n",
    "            if c == \"SEP\":\n",
    "                v += 1\n",
    "\n",
    "        code_tokens = (codes + [\"PAD\"]*self.max_len)[:self.max_len]\n",
    "        _, code_ids = code2index(np.array(codes), self.vocab)\n",
    "        code_ids = seq_padding(code_ids, self.max_len, symbol=self.pad_id)\n",
    "\n",
    "        attMask = (np.array(code_ids) != self.pad_id).astype(np.float32)\n",
    "\n",
    "        return (\n",
    "            torch.LongTensor(code_ids),\n",
    "            torch.FloatTensor(attMask),\n",
    "            torch.LongTensor((visit_ids + [-1]*(self.max_len-len(visit_ids)))[:self.max_len]),\n",
    "            code_tokens,\n",
    "            self.label[idx],\n",
    "            int(self.patid[idx])\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.code)\n",
    "\n",
    "def collate_explain(batch):\n",
    "    code_ids, att, visit_ids, code_tokens, labels, patids = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(code_ids),\n",
    "        torch.stack(att),\n",
    "        torch.stack(visit_ids),\n",
    "        list(code_tokens),\n",
    "        list(labels),\n",
    "        list(patids)\n",
    "    )\n",
    "\n",
    "testset_explain = NextVisitExplain(BertVocab[\"token2idx\"], test_df, global_params[\"max_len_seq\"])\n",
    "testload_explain = DataLoader(testset_explain, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_explain)\n",
    "\n",
    "print(\"✅ testload_explain ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55950690",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(testload_explain))\n",
    "code_ids, attMask, visit_ids, code_tokens, labels, patids = batch\n",
    "\n",
    "print(\"code_ids:\", code_ids.shape)\n",
    "print(\"visit_ids:\", visit_ids.shape)\n",
    "print(\"code_tokens[0][:20]:\", code_tokens[0][:20])\n",
    "print(\"patid[0]:\", patids[0], \"| attSum:\", attMask[0].sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "def create_folder(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "OUT_DIR = global_params.get(\"output_dir\", \"./outputs\")\n",
    "create_folder(OUT_DIR)\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_micro_metrics(y_true, y_logits):\n",
    "    \"\"\"\n",
    "    y_true: [N, C] 0/1\n",
    "    y_logits: [N, C] raw logits\n",
    "    returns: (APS_micro, AUROC_micro)\n",
    "    \"\"\"\n",
    "    y_prob = sigmoid_np(y_logits)\n",
    "    aps = average_precision_score(y_true, y_prob, average=\"micro\")\n",
    "    auroc = roc_auc_score(y_true, y_prob, average=\"micro\")\n",
    "    return aps, auroc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "aps_list = []\n",
    "auroc_list = []\n",
    "loss_list = []\n",
    "\n",
    "best_aps = -1\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    tr_loss = train_one_epoch(e, log_every=200)  # 你已有的 train func\n",
    "    aps, auroc = evaluation()                    # 你已有的 eval func (回傳 aps, auroc)\n",
    "\n",
    "    loss_list.append(float(tr_loss))\n",
    "    aps_list.append(float(aps))\n",
    "    auroc_list.append(float(auroc))\n",
    "\n",
    "    print(f\"==> epoch {e:02d}: train_loss={tr_loss:.4f} APS(micro)={aps:.4f} AUROC(micro)={auroc:.4f}\")\n",
    "\n",
    "    # 存 best model（可選）\n",
    "    if aps > best_aps and global_params.get(\"save_model\", True):\n",
    "        best_aps = aps\n",
    "        ckpt_path = os.path.join(OUT_DIR, global_params.get(\"best_name\", \"best.pt\"))\n",
    "        torch.save((model.module if hasattr(model, \"module\") else model).state_dict(), ckpt_path)\n",
    "        print(\"✅ Saved best ckpt:\", ckpt_path)\n",
    "\n",
    "# --- plot APS vs epoch ---\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(EPOCHS), aps_list, marker='o')\n",
    "plt.title(\"NextVisit-12m: APS vs Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"APS (micro)\")\n",
    "plt.grid(True)\n",
    "\n",
    "aps_png = os.path.join(OUT_DIR, \"nextvisit_aps_vs_epoch.png\")\n",
    "plt.savefig(aps_png, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✅ Saved:\", aps_png)\n",
    "\n",
    "# --- plot AUROC vs epoch ---\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(EPOCHS), auroc_list, marker='s')\n",
    "plt.title(\"NextVisit-12m: AUROC vs Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"AUROC (micro)\")\n",
    "plt.grid(True)\n",
    "\n",
    "auroc_png = os.path.join(OUT_DIR, \"nextvisit_auroc_vs_epoch.png\")\n",
    "plt.savefig(auroc_png, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✅ Saved:\", auroc_png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2355ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_prob = collect_test_probs()\n",
    "\n",
    "# ===== micro flatten =====\n",
    "y_true_micro = y_true.ravel()\n",
    "y_prob_micro = y_prob.ravel()\n",
    "\n",
    "# ===== PR curve =====\n",
    "prec, rec, _ = precision_recall_curve(y_true_micro, y_prob_micro)\n",
    "ap_micro = average_precision_score(y_true_micro, y_prob_micro)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec)\n",
    "plt.title(f\"PR Curve (micro) | AP={ap_micro:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid(True)\n",
    "\n",
    "pr_png = os.path.join(OUT_DIR, \"nextvisit_pr_curve_micro.png\")\n",
    "plt.savefig(pr_png, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✅ Saved:\", pr_png)\n",
    "\n",
    "# ===== ROC curve =====\n",
    "fpr, tpr, _ = roc_curve(y_true_micro, y_prob_micro)\n",
    "auc_micro = auc(fpr, tpr)  # 或 roc_auc_score(y_true_micro, y_prob_micro)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title(f\"ROC Curve (micro) | AUC={auc_micro:.4f}\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.grid(True)\n",
    "\n",
    "roc_png = os.path.join(OUT_DIR, \"nextvisit_roc_curve_micro.png\")\n",
    "plt.savefig(roc_png, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✅ Saved:\", roc_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch Baseline Logistic Regression (multi-label, GPU support) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Use same X_train, X_test, Y_train, Y_test as above\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test_torch  = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "Y_train_torch = torch.tensor(Y_train, dtype=torch.float32).to(device)\n",
    "Y_test_torch  = torch.tensor(Y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_labels = Y_train.shape[1]\n",
    "\n",
    "class TorchLogReg(nn.Module):\n",
    "    def __init__(self, n_features, n_labels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(n_features, n_labels)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = TorchLogReg(n_features, n_labels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "EPOCHS = 15\n",
    "batch_size = 128\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    perm = torch.randperm(X_train_torch.size(0), device=device)\n",
    "    total_loss = 0.0\n",
    "    for i in range(0, X_train_torch.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        xb = X_train_torch[idx]\n",
    "        yb = Y_train_torch[idx]\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / X_train_torch.size(0)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_torch)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    y_true = Y_test_torch.cpu().numpy()\n",
    "\n",
    "aps_micro = average_precision_score(y_true, probs, average='micro')\n",
    "auroc_micro = roc_auc_score(y_true, probs, average='micro')\n",
    "print(f\"[PyTorch-LogReg] APS = {aps_micro:.4f}, AUROC = {auroc_micro:.4f}\")\n",
    "\n",
    "OUT_DIR = \"../../outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "PR_CURVE_PNG  = os.path.join(OUT_DIR, \"baseline_torchlogreg_pr.png\")\n",
    "ROC_CURVE_PNG = os.path.join(OUT_DIR, \"baseline_torchlogreg_roc.png\")\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true.ravel(), probs.ravel())\n",
    "fpr, tpr, _ = roc_curve(y_true.ravel(), probs.ravel())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f\"AP={aps_micro:.3f}\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR Curve (PyTorch LogReg Baseline)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(PR_CURVE_PNG, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auroc_micro:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", alpha=0.5)\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (PyTorch LogReg Baseline)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROC_CURVE_PNG, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"PR →\", PR_CURVE_PNG)\n",
    "print(\"ROC →\", ROC_CURVE_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f363a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, roc_curve, auc,\n",
    "    average_precision_score, roc_auc_score\n",
    ")\n",
    "\n",
    "def micro_flat(y_true, y_prob):\n",
    "    return y_true.ravel(), y_prob.ravel()\n",
    "\n",
    "def pr_roc_stats(y_true_micro, y_prob_micro):\n",
    "    # PR\n",
    "    precision, recall, _ = precision_recall_curve(y_true_micro, y_prob_micro)\n",
    "    ap = average_precision_score(y_true_micro, y_prob_micro)\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true_micro, y_prob_micro)\n",
    "    roc_auc = auc(fpr, tpr)  # same as roc_auc_score for binary micro-flatten\n",
    "    return (precision, recall, ap), (fpr, tpr, roc_auc)\n",
    "\n",
    "# =========================\n",
    "# 1) Prepare inputs\n",
    "# =========================\n",
    "# --- BEHRT ---\n",
    "y_true_behrt, y_prob_behrt = collect_test_probs()   # if already computed, reuse it\n",
    "y_true_behrt_micro, y_prob_behrt_micro = micro_flat(y_true_behrt, y_prob_behrt)\n",
    "\n",
    "# --- LogReg baseline ---\n",
    "# rename your baseline variables to avoid overwrite confusion:\n",
    "# y_true_lr = y_true\n",
    "# probs_lr  = probs\n",
    "y_true_lr_micro, probs_lr_micro = micro_flat(y_true_lr, probs_lr)\n",
    "\n",
    "# (Optional sanity check) same length?\n",
    "assert y_true_behrt_micro.shape == y_true_lr_micro.shape, \\\n",
    "    f\"Mismatch: BEHRT {y_true_behrt_micro.shape} vs LR {y_true_lr_micro.shape}\"\n",
    "\n",
    "# =========================\n",
    "# 2) Compute curves\n",
    "# =========================\n",
    "(behrt_prec, behrt_rec, behrt_ap), (behrt_fpr, behrt_tpr, behrt_auc) = pr_roc_stats(\n",
    "    y_true_behrt_micro, y_prob_behrt_micro\n",
    ")\n",
    "\n",
    "(lr_prec, lr_rec, lr_ap), (lr_fpr, lr_tpr, lr_auc) = pr_roc_stats(\n",
    "    y_true_lr_micro, probs_lr_micro\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 3) Plot comparison\n",
    "# =========================\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "PR_CMP_PNG  = os.path.join(OUT_DIR, \"compare_pr_micro_behrt_vs_logreg.png\")\n",
    "ROC_CMP_PNG = os.path.join(OUT_DIR, \"compare_roc_micro_behrt_vs_logreg.png\")\n",
    "\n",
    "# ---- PR comparison ----\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(behrt_rec, behrt_prec, label=f\"BEHRT (AP={behrt_ap:.4f})\")\n",
    "plt.plot(lr_rec, lr_prec, label=f\"BoC+LogReg (AP={lr_ap:.4f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR Curve (micro) — BEHRT vs Baseline\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(PR_CMP_PNG, dpi=200)\n",
    "plt.show()\n",
    "print(\"✅ Saved:\", PR_CMP_PNG)\n",
    "\n",
    "# ---- ROC comparison ----\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(behrt_fpr, behrt_tpr, label=f\"BEHRT (AUC={behrt_auc:.4f})\")\n",
    "plt.plot(lr_fpr, lr_tpr, label=f\"BoC+LogReg (AUC={lr_auc:.4f})\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (micro) — BEHRT vs Baseline\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROC_CMP_PNG, dpi=200)\n",
    "plt.show()\n",
    "print(\"✅ Saved:\", ROC_CMP_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cdf48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BD4H",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
